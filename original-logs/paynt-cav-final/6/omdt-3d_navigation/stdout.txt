2025-01-21 19:16:04,607 - cli.py - This is Paynt version 0.1.0.
2025-01-21 19:16:04,607 - sketch.py - loading sketch from /home/imacak/disk2/synthesis/models/dts/omdt/3d_navigation/model-random.drn ...
2025-01-21 19:16:04,607 - sketch.py - assuming sketch in PRISM format...
ERROR (SpiritErrorHandler.h:26): Parsing error at 3:1:  expecting <model type>, here:
	@type: MDP
	^

2025-01-21 19:16:04,610 - sketch.py - assuming sketch in DRN format...
2025-01-21 19:16:04,611 - prism_parser.py - loading properties from /home/imacak/disk2/synthesis/models/dts/omdt/3d_navigation/discounted.props ...
2025-01-21 19:16:04,612 - prism_parser.py - found the following specification: optimality: R{"reward"}max=? [Cdiscount=99/100] 
2025-01-21 19:16:04,612 - sketch.py - found state valuations in /home/imacak/disk2/synthesis/models/dts/omdt/3d_navigation/state-valuations.json, adding to the model...
2025-01-21 19:16:04,612 - sketch.py - sketch parsing OK
2025-01-21 19:16:04,612 - sketch.py - constructed explicit quotient having 125 states and 875 choices
2025-01-21 19:16:04,612 - sketch.py - found the following specification optimality: R{"reward"}max=? [Cdiscount=99/100] 
2025-01-21 19:16:04,613 - mdp.py - MDP has 125/125 relevant states
2025-01-21 19:16:04,614 - mdp.py - MDP has 7 actions
2025-01-21 19:16:04,614 - mdp.py - found the following 3 variables: ['x:[0..4]', 'y:[0..4]', 'z:[0..4]']
2025-01-21 19:16:04,618 - decision_tree.py - the optimal scheduler has value: 0.3518477922915202
2025-01-21 19:16:04,619 - decision_tree.py - the random scheduler has value: 1.1945613532024873e-06

2025-01-21 19:16:04,619 - mdp.py - building tree of depth 0
2025-01-21 19:16:04,623 - statistic.py - synthesis initiated, design space: 7
2025-01-21 19:16:04,626 - synthesizer_ar.py - value 0.0 achieved after 0.02 seconds
--------------------
Synthesis summary:
optimality objective: R{"reward"}max=? [Cdiscount=99/100] 

method: AR (decision tree), synthesis time: 0.01 s
number of holes: 1, family size: 7, quotient: 125 states / 875 actions
explored: 100 %
MDP stats: avg MDP size: 65, iterations: 4

optimum: 0.0
--------------------
2025-01-21 19:16:04,628 - decision_tree.py - families considered: 4
2025-01-21 19:16:04,628 - decision_tree.py - families skipped by construction: 0
2025-01-21 19:16:04,628 - decision_tree.py - families with schedulers preserved: 0
2025-01-21 19:16:04,628 - decision_tree.py - families model checked: 4
2025-01-21 19:16:04,628 - decision_tree.py - harmonizations attempted: 1
2025-01-21 19:16:04,628 - decision_tree.py - harmonizations succeeded: 1

2025-01-21 19:16:04,628 - decision_tree.py - printing synthesized assignment below:
2025-01-21 19:16:04,628 - decision_tree.py - A_0=(forward)
2025-01-21 19:16:04,628 - decision_tree.py - double-checking specification satisfiability:  : 0.0

2025-01-21 19:16:04,628 - mdp.py - building tree of depth 1
2025-01-21 19:16:04,637 - statistic.py - synthesis initiated, design space: 9408
--------------------
Synthesis summary:
optimality objective: R{"reward"}max=? [Cdiscount=99/100] 

method: AR (decision tree), synthesis time: 0.28 s
number of holes: 6, family size: 9408, quotient: 125 states / 875 actions
explored: 200 %
MDP stats: avg MDP size: 87, iterations: 180

optimum: 0.0
--------------------
2025-01-21 19:16:04,914 - decision_tree.py - families considered: 180
2025-01-21 19:16:04,915 - decision_tree.py - families skipped by construction: 0
2025-01-21 19:16:04,915 - decision_tree.py - families with schedulers preserved: 20
2025-01-21 19:16:04,915 - decision_tree.py - families model checked: 160
2025-01-21 19:16:04,915 - decision_tree.py - harmonizations attempted: 44
2025-01-21 19:16:04,915 - decision_tree.py - harmonizations succeeded: 0


2025-01-21 19:16:04,915 - mdp.py - building tree of depth 2
2025-01-21 19:16:04,928 - statistic.py - synthesis initiated, design space: 1e10
2025-01-21 19:16:05,008 - synthesizer_ar.py - value 0.0005 achieved after 0.4 seconds
2025-01-21 19:16:06,616 - synthesizer_ar.py - value 0.001 achieved after 2.01 seconds
2025-01-21 19:16:06,884 - synthesizer_ar.py - value 0.0014 achieved after 2.28 seconds
> progress 5.487%, elapsed 3 s, estimated 54 s, iters = {MDP: 1374}, opt = 0.0014
2025-01-21 19:16:08,140 - synthesizer_ar.py - value 0.0121 achieved after 3.53 seconds
2025-01-21 19:16:08,896 - synthesizer_ar.py - value 0.0123 achieved after 4.29 seconds
> progress 8.252%, elapsed 6 s, estimated 72 s, iters = {MDP: 2688}, opt = 0.0123
2025-01-21 19:16:11,120 - synthesizer_ar.py - value 0.0139 achieved after 6.51 seconds
> progress 9.511%, elapsed 9 s, estimated 94 s, iters = {MDP: 4000}, opt = 0.0139
> progress 24.822%, elapsed 12 s, estimated 48 s, iters = {MDP: 5437}, opt = 0.0139
> progress 38.586%, elapsed 15 s, estimated 38 s, iters = {MDP: 6577}, opt = 0.0139
> progress 44.828%, elapsed 18 s, estimated 40 s, iters = {MDP: 7756}, opt = 0.0139
> progress 46.571%, elapsed 21 s, estimated 45 s, iters = {MDP: 8693}, opt = 0.0139
> progress 52.931%, elapsed 24 s, estimated 45 s, iters = {MDP: 9721}, opt = 0.0139
> progress 56.741%, elapsed 27 s, estimated 47 s, iters = {MDP: 10953}, opt = 0.0139
> progress 59.042%, elapsed 30 s, estimated 50 s, iters = {MDP: 12177}, opt = 0.0139
> progress 67.39%, elapsed 33 s, estimated 49 s, iters = {MDP: 13486}, opt = 0.0139
2025-01-21 19:16:38,085 - synthesizer_ar.py - value 0.0141 achieved after 33.48 seconds
> progress 69.071%, elapsed 36 s, estimated 52 s, iters = {MDP: 14473}, opt = 0.0141
2025-01-21 19:16:42,883 - synthesizer_ar.py - value 0.0207 achieved after 38.28 seconds
> progress 71.212%, elapsed 39 s, estimated 54 s, iters = {MDP: 15569}, opt = 0.0207
> progress 81.484%, elapsed 42 s, estimated 51 s, iters = {MDP: 16722}, opt = 0.0207
> progress 91.591%, elapsed 45 s, estimated 49 s, iters = {MDP: 17683}, opt = 0.0207
> progress 102.267%, elapsed 48 s, estimated 46 s, iters = {MDP: 18828}, opt = 0.0207
> progress 112.553%, elapsed 51 s, estimated 45 s, iters = {MDP: 20041}, opt = 0.0207
> progress 126.35%, elapsed 54 s, estimated 42 s, iters = {MDP: 21267}, opt = 0.0207
> progress 132.857%, elapsed 57 s, estimated 42 s, iters = {MDP: 22512}, opt = 0.0207
> progress 146.173%, elapsed 60 s, estimated 41 s, iters = {MDP: 23798}, opt = 0.0207
> progress 151.858%, elapsed 63 s, estimated 41 s, iters = {MDP: 25163}, opt = 0.0207
> progress 164.345%, elapsed 66 s, estimated 40 s, iters = {MDP: 26346}, opt = 0.0207
> progress 177.843%, elapsed 69 s, estimated 38 s, iters = {MDP: 27553}, opt = 0.0207
> progress 184.36%, elapsed 72 s, estimated 39 s, iters = {MDP: 28678}, opt = 0.0207
> progress 191.967%, elapsed 75 s, estimated 39 s, iters = {MDP: 29876}, opt = 0.0207
--------------------
Synthesis summary:
optimality objective: R{"reward"}max=? [Cdiscount=99/100] 

method: AR (decision tree), synthesis time: 77.14 s
number of holes: 16, family size: 1e10, quotient: 125 states / 875 actions
explored: 200 %
MDP stats: avg MDP size: 94, iterations: 30651

optimum: 0.020666
--------------------
2025-01-21 19:17:22,063 - decision_tree.py - families considered: 30651
2025-01-21 19:17:22,063 - decision_tree.py - families skipped by construction: 0
2025-01-21 19:17:22,064 - decision_tree.py - families with schedulers preserved: 5130
2025-01-21 19:17:22,064 - decision_tree.py - families model checked: 25521
2025-01-21 19:17:22,064 - decision_tree.py - harmonizations attempted: 6756
2025-01-21 19:17:22,064 - decision_tree.py - harmonizations succeeded: 5

2025-01-21 19:17:22,064 - decision_tree.py - printing synthesized assignment below:
2025-01-21 19:17:22,064 - decision_tree.py - V_0=x, x_0=3, y_0=0, z_0=0, V_1=y, x_1=0, y_1=0, z_1=0, A_2=(up), A_3=(right), V_4=z, x_4=3, y_4=0, z_4=3, A_5=(forward), A_6=(up)
2025-01-21 19:17:22,064 - decision_tree.py - double-checking specification satisfiability:  : 0.0206659002845865

2025-01-21 19:17:22,064 - mdp.py - building tree of depth 3
2025-01-21 19:17:22,088 - statistic.py - synthesis initiated, design space: 1e22
2025-01-21 19:17:23,005 - synthesizer_ar.py - value 0.0221 achieved after 78.4 seconds
2025-01-21 19:17:23,115 - synthesizer_ar.py - value 0.0307 achieved after 78.51 seconds
> progress 0.024%, elapsed 3 s, estimated 12135 s (3 hours), iters = {MDP: 892}, opt = 0.0307
> progress 0.03%, elapsed 6 s, estimated 19915 s (5 hours), iters = {MDP: 1618}, opt = 0.0307
2025-01-21 19:17:29,001 - synthesizer_ar.py - value 0.049 achieved after 84.39 seconds
> progress 0.054%, elapsed 9 s, estimated 16683 s (4 hours), iters = {MDP: 2516}, opt = 0.049
2025-01-21 19:17:31,434 - synthesizer_ar.py - value 0.3518 achieved after 86.83 seconds
--------------------
Synthesis summary:
optimality objective: R{"reward"}max=? [Cdiscount=99/100] 

method: AR (decision tree), synthesis time: 9.35 s
number of holes: 36, family size: 1e22, quotient: 125 states / 875 actions
explored: 100 %
MDP stats: avg MDP size: 102, iterations: 2604

optimum: 0.351848
--------------------
2025-01-21 19:17:31,434 - decision_tree.py - families considered: 2604
2025-01-21 19:17:31,434 - decision_tree.py - families skipped by construction: 0
2025-01-21 19:17:31,434 - decision_tree.py - families with schedulers preserved: 543
2025-01-21 19:17:31,434 - decision_tree.py - families model checked: 2061
2025-01-21 19:17:31,434 - decision_tree.py - harmonizations attempted: 424
2025-01-21 19:17:31,434 - decision_tree.py - harmonizations succeeded: 0

2025-01-21 19:17:31,434 - decision_tree.py - printing synthesized assignment below:
2025-01-21 19:17:31,434 - decision_tree.py - V_0=x, x_0=2, y_0=0, z_0=3, V_1=z, x_1=2, y_1=0, z_1=0, V_2=x, x_2=1, y_2=0, z_2=3, A_3=(right), A_4=(forward), V_5=z, x_5=0, y_5=0, z_5=3, A_6=(right), A_7=(backward), V_8=y, x_8=0, y_8=3, z_8=0, V_9=z, x_9=0, y_9=3, z_9=2, A_10=(forward), A_11=(up), V_12=z, x_12=0, y_12=0, z_12=3, A_13=(forward), A_14=(right)
2025-01-21 19:17:31,435 - decision_tree.py - double-checking specification satisfiability:  : 0.35184781396800185
2025-01-21 19:17:31,435 - decision_tree.py - the optimal scheduler has value: 0.3518477922915202
2025-01-21 19:17:31,435 - decision_tree.py - the random scheduler has value: 1.1945613532024873e-06
2025-01-21 19:17:31,435 - decision_tree.py - synthesized tree of depth 3 with 7 decision nodes
2025-01-21 19:17:31,435 - decision_tree.py - the synthesized tree has value 0.35184781396800185
2025-01-21 19:17:31,435 - decision_tree.py - the synthesized tree has relative value: 1.0000000616077627
2025-01-21 19:17:31,435 - decision_tree.py - printing the synthesized tree below:
if x<=2:
  if z<=0:
    if x<=1:
      (right)
    else:
      (forward)
  else:
    if z<=3:
      (right)
    else:
      (backward)
else:
  if y<=3:
    if z<=2:
      (forward)
    else:
      (up)
  else:
    if z<=3:
      (forward)
    else:
      (right)

2025-01-21 19:17:31,436 - decision_tree.py - exported decision tree to logs/01-21-all/6/omdt-3d_navigation/tree.dot
2025-01-21 19:17:31,491 - decision_tree.py - exported decision tree visualization to logs/01-21-all/6/omdt-3d_navigation/tree.png
2025-01-21 19:17:31,491 - decision_tree.py - synthesis finished after 86.88 seconds

ColoringSmt = 0.017 s (0.0 %)
ColoringSmt:: create choice colors = 0.004 s (0.0 %)
ColoringSmt:: create harmonizing variants = 0.012 s (0.0 %)
areChoicesConsistent = 4.825 s (5.6 %)
areChoicesConsistent::1 is scheduler consistent? = 1.726 s (2.0 %)
areChoicesConsistent::2 better unsat core = 2.09 s (2.4 %)
areChoicesConsistent::3 unsat core analysis = 0.965 s (1.1 %)
check = 4.794 s (5.5 %)
loadUnsatCore = 0.005 s (0.0 %)
selectCompatibleChoices = 3.002 s (3.5 %)
selectCompatibleChoices::1 is family sat = 2.851 s (3.3 %)
selectCompatibleChoices::2 state exploration = 0.149 s (0.2 %)
