2025-01-21 17:45:15,840 - cli.py - This is Paynt version 0.1.0.
2025-01-21 17:45:15,841 - sketch.py - loading sketch from /home/imacak/disk2/synthesis/models/dts/omdt/blackjack/model-random.drn ...
2025-01-21 17:45:15,841 - sketch.py - assuming sketch in PRISM format...
ERROR (SpiritErrorHandler.h:26): Parsing error at 3:1:  expecting <model type>, here:
	@type: MDP
	^

2025-01-21 17:45:15,849 - sketch.py - assuming sketch in DRN format...
2025-01-21 17:45:15,855 - prism_parser.py - loading properties from /home/imacak/disk2/synthesis/models/dts/omdt/blackjack/discounted.props ...
2025-01-21 17:45:15,855 - prism_parser.py - found the following specification: optimality: R{"reward"}max=? [Cdiscount=99/100] 
2025-01-21 17:45:15,856 - sketch.py - found state valuations in /home/imacak/disk2/synthesis/models/dts/omdt/blackjack/state-valuations.json, adding to the model...
2025-01-21 17:45:15,857 - sketch.py - sketch parsing OK
2025-01-21 17:45:15,857 - sketch.py - constructed explicit quotient having 533 states and 1599 choices
2025-01-21 17:45:15,857 - sketch.py - found the following specification optimality: R{"reward"}max=? [Cdiscount=99/100] 
2025-01-21 17:45:15,859 - mdp.py - MDP has 533/533 relevant states
2025-01-21 17:45:15,860 - mdp.py - MDP has 3 actions
2025-01-21 17:45:15,862 - mdp.py - found the following 3 variables: ['dealer_total:[0..22]', 'player_total:[0..22]', 'skipped:[0..1]']
2025-01-21 17:45:15,868 - decision_tree.py - the optimal scheduler has value: -2.1731936652535744
2025-01-21 17:45:15,869 - decision_tree.py - the random scheduler has value: -11.731985750064817

2025-01-21 17:45:15,869 - mdp.py - building tree of depth 0
2025-01-21 17:45:15,876 - statistic.py - synthesis initiated, design space: 3
2025-01-21 17:45:15,883 - synthesizer_ar.py - value -27.8078 achieved after 0.04 seconds
2025-01-21 17:45:15,884 - synthesizer_ar.py - value -4.122 achieved after 0.04 seconds
--------------------
Synthesis summary:
optimality objective: R{"reward"}max=? [Cdiscount=99/100] 

method: AR (decision tree), synthesis time: 0.01 s
number of holes: 1, family size: 3, quotient: 533 states / 1599 actions
explored: 100 %
MDP stats: avg MDP size: 448, iterations: 4

optimum: -4.122045
--------------------
2025-01-21 17:45:15,887 - decision_tree.py - families considered: 4
2025-01-21 17:45:15,887 - decision_tree.py - families skipped by construction: 0
2025-01-21 17:45:15,887 - decision_tree.py - families with schedulers preserved: 0
2025-01-21 17:45:15,887 - decision_tree.py - families model checked: 4
2025-01-21 17:45:15,887 - decision_tree.py - harmonizations attempted: 1
2025-01-21 17:45:15,887 - decision_tree.py - harmonizations succeeded: 2

2025-01-21 17:45:15,887 - decision_tree.py - printing synthesized assignment below:
2025-01-21 17:45:15,887 - decision_tree.py - A_0=(Skip)
2025-01-21 17:45:15,889 - decision_tree.py - double-checking specification satisfiability:  : -4.1220450006834595

2025-01-21 17:45:15,889 - mdp.py - building tree of depth 1
2025-01-21 17:45:15,906 - statistic.py - synthesis initiated, design space: 10773
2025-01-21 17:45:15,992 - synthesizer_ar.py - value -2.3954 achieved after 0.15 seconds
2025-01-21 17:45:16,002 - synthesizer_ar.py - value -2.3526 achieved after 0.16 seconds
--------------------
Synthesis summary:
optimality objective: R{"reward"}max=? [Cdiscount=99/100] 

method: AR (decision tree), synthesis time: 0.73 s
number of holes: 6, family size: 10773, quotient: 533 states / 1599 actions
explored: 200 %
MDP stats: avg MDP size: 480, iterations: 116

optimum: -2.352597
--------------------
2025-01-21 17:45:16,633 - decision_tree.py - families considered: 116
2025-01-21 17:45:16,633 - decision_tree.py - families skipped by construction: 0
2025-01-21 17:45:16,633 - decision_tree.py - families with schedulers preserved: 29
2025-01-21 17:45:16,633 - decision_tree.py - families model checked: 87
2025-01-21 17:45:16,633 - decision_tree.py - harmonizations attempted: 16
2025-01-21 17:45:16,633 - decision_tree.py - harmonizations succeeded: 1

2025-01-21 17:45:16,633 - decision_tree.py - printing synthesized assignment below:
2025-01-21 17:45:16,633 - decision_tree.py - V_0=player_total, dealer_total_0=7, player_total_0=11, skipped_0=0, A_1=(Draw), A_2=(Skip)
2025-01-21 17:45:16,634 - decision_tree.py - double-checking specification satisfiability:  : -2.3525971558866057

2025-01-21 17:45:16,634 - mdp.py - building tree of depth 2
2025-01-21 17:45:16,670 - statistic.py - synthesis initiated, design space: 1e11
> progress 12.655%, elapsed 3 s, estimated 23 s, iters = {MDP: 243}, opt = -2.3526
> progress 16.268%, elapsed 6 s, estimated 36 s, iters = {MDP: 475}, opt = -2.3526
> progress 22.742%, elapsed 9 s, estimated 39 s, iters = {MDP: 732}, opt = -2.3526
> progress 28.817%, elapsed 12 s, estimated 41 s, iters = {MDP: 977}, opt = -2.3526
> progress 30.671%, elapsed 15 s, estimated 49 s, iters = {MDP: 1194}, opt = -2.3526
> progress 32.102%, elapsed 18 s, estimated 56 s, iters = {MDP: 1447}, opt = -2.3526
> progress 34.857%, elapsed 21 s, estimated 60 s, iters = {MDP: 1693}, opt = -2.3526
2025-01-21 17:45:39,654 - synthesizer_ar.py - value -2.3199 achieved after 23.81 seconds
2025-01-21 17:45:39,671 - synthesizer_ar.py - value -2.2792 achieved after 23.83 seconds
2025-01-21 17:45:39,687 - synthesizer_ar.py - value -2.2326 achieved after 23.85 seconds
2025-01-21 17:45:39,891 - synthesizer_ar.py - value -2.2053 achieved after 24.05 seconds
> progress 48.556%, elapsed 24 s, estimated 49 s, iters = {MDP: 1922}, opt = -2.2053
> progress 55.823%, elapsed 27 s, estimated 48 s, iters = {MDP: 2165}, opt = -2.2053
> progress 67.186%, elapsed 30 s, estimated 44 s, iters = {MDP: 2406}, opt = -2.2053
> progress 86.064%, elapsed 33 s, estimated 38 s, iters = {MDP: 2655}, opt = -2.2053
> progress 98.018%, elapsed 36 s, estimated 36 s, iters = {MDP: 2922}, opt = -2.2053
--------------------
Synthesis summary:
optimality objective: R{"reward"}max=? [Cdiscount=99/100] 

method: AR (decision tree), synthesis time: 37.57 s
number of holes: 16, family size: 1e11, quotient: 533 states / 1599 actions
explored: 101 %
MDP stats: avg MDP size: 504, iterations: 3040

optimum: -2.205256
--------------------
2025-01-21 17:45:54,240 - decision_tree.py - families considered: 3040
2025-01-21 17:45:54,240 - decision_tree.py - families skipped by construction: 0
2025-01-21 17:45:54,240 - decision_tree.py - families with schedulers preserved: 947
2025-01-21 17:45:54,240 - decision_tree.py - families model checked: 2093
2025-01-21 17:45:54,240 - decision_tree.py - harmonizations attempted: 292
2025-01-21 17:45:54,240 - decision_tree.py - harmonizations succeeded: 1

2025-01-21 17:45:54,240 - decision_tree.py - printing synthesized assignment below:
2025-01-21 17:45:54,240 - decision_tree.py - V_0=dealer_total, dealer_total_0=6, player_total_0=0, skipped_0=0, V_1=player_total, dealer_total_1=0, player_total_1=11, skipped_1=0, A_2=(Draw), A_3=(Skip), V_4=player_total, dealer_total_4=11, player_total_4=13, skipped_4=0, A_5=(Draw), A_6=(Skip)
2025-01-21 17:45:54,241 - decision_tree.py - double-checking specification satisfiability:  : -2.2052559113533015

2025-01-21 17:45:54,241 - mdp.py - building tree of depth 3
2025-01-21 17:45:54,318 - statistic.py - synthesis initiated, design space: 1e25
> progress 0.0%, elapsed 3 s, estimated 1049455 s (12 days), iters = {MDP: 193}, opt = -2.2053
> progress 0.0%, elapsed 6 s, estimated 1354234 s (15 days), iters = {MDP: 382}, opt = -2.2053
> progress 0.204%, elapsed 9 s, estimated 4419 s, iters = {MDP: 505}, opt = -2.2053
> progress 0.25%, elapsed 12 s, estimated 4818 s, iters = {MDP: 635}, opt = -2.2053
> progress 0.303%, elapsed 15 s, estimated 4976 s, iters = {MDP: 774}, opt = -2.2053
> progress 0.379%, elapsed 18 s, estimated 4777 s, iters = {MDP: 912}, opt = -2.2053
> progress 0.56%, elapsed 21 s, estimated 3774 s, iters = {MDP: 1053}, opt = -2.2053
> progress 0.602%, elapsed 24 s, estimated 4014 s, iters = {MDP: 1202}, opt = -2.2053
> progress 0.771%, elapsed 27 s, estimated 3526 s, iters = {MDP: 1347}, opt = -2.2053
> progress 0.804%, elapsed 30 s, estimated 3759 s, iters = {MDP: 1489}, opt = -2.2053
> progress 0.863%, elapsed 33 s, estimated 3856 s, iters = {MDP: 1645}, opt = -2.2053
> progress 0.872%, elapsed 36 s, estimated 4160 s, iters = {MDP: 1785}, opt = -2.2053
> progress 0.886%, elapsed 39 s, estimated 4437 s, iters = {MDP: 1940}, opt = -2.2053
> progress 0.919%, elapsed 42 s, estimated 4608 s, iters = {MDP: 2083}, opt = -2.2053
> progress 0.962%, elapsed 45 s, estimated 4714 s, iters = {MDP: 2245}, opt = -2.2053
> progress 1.113%, elapsed 48 s, estimated 4344 s, iters = {MDP: 2354}, opt = -2.2053
> progress 1.115%, elapsed 51 s, estimated 4612 s, iters = {MDP: 2490}, opt = -2.2053
> progress 1.115%, elapsed 54 s, estimated 4884 s, iters = {MDP: 2621}, opt = -2.2053
> progress 1.115%, elapsed 57 s, estimated 5154 s, iters = {MDP: 2756}, opt = -2.2053
> progress 1.115%, elapsed 60 s, estimated 5424 s, iters = {MDP: 2898}, opt = -2.2053
> progress 1.115%, elapsed 63 s, estimated 5695 s, iters = {MDP: 3039}, opt = -2.2053
> progress 1.115%, elapsed 66 s, estimated 5966 s, iters = {MDP: 3190}, opt = -2.2053
> progress 1.115%, elapsed 69 s, estimated 6238 s, iters = {MDP: 3327}, opt = -2.2053
> progress 1.115%, elapsed 72 s, estimated 6507 s, iters = {MDP: 3480}, opt = -2.2053
> progress 1.115%, elapsed 75 s, estimated 6777 s, iters = {MDP: 3604}, opt = -2.2053
> progress 1.115%, elapsed 78 s, estimated 7052 s, iters = {MDP: 3737}, opt = -2.2053
> progress 1.115%, elapsed 81 s, estimated 7325 s (2 hours), iters = {MDP: 3869}, opt = -2.2053
> progress 1.115%, elapsed 84 s, estimated 7594 s (2 hours), iters = {MDP: 4001}, opt = -2.2053
> progress 1.115%, elapsed 87 s, estimated 7866 s (2 hours), iters = {MDP: 4142}, opt = -2.2053
> progress 1.115%, elapsed 90 s, estimated 8135 s (2 hours), iters = {MDP: 4296}, opt = -2.2053
> progress 1.115%, elapsed 93 s, estimated 8405 s (2 hours), iters = {MDP: 4451}, opt = -2.2053
> progress 1.116%, elapsed 96 s, estimated 8670 s (2 hours), iters = {MDP: 4595}, opt = -2.2053
> progress 1.116%, elapsed 99 s, estimated 8939 s (2 hours), iters = {MDP: 4778}, opt = -2.2053
> progress 1.116%, elapsed 102 s, estimated 9210 s (2 hours), iters = {MDP: 4960}, opt = -2.2053
> progress 1.116%, elapsed 105 s, estimated 9481 s (2 hours), iters = {MDP: 5131}, opt = -2.2053
> progress 1.116%, elapsed 108 s, estimated 9754 s (2 hours), iters = {MDP: 5317}, opt = -2.2053
> progress 1.116%, elapsed 111 s, estimated 10023 s (2 hours), iters = {MDP: 5459}, opt = -2.2053
> progress 1.116%, elapsed 114 s, estimated 10293 s (2 hours), iters = {MDP: 5636}, opt = -2.2053
> progress 1.116%, elapsed 117 s, estimated 10562 s (2 hours), iters = {MDP: 5802}, opt = -2.2053
> progress 1.116%, elapsed 120 s, estimated 10832 s (3 hours), iters = {MDP: 5950}, opt = -2.2053
> progress 1.116%, elapsed 123 s, estimated 11103 s (3 hours), iters = {MDP: 6121}, opt = -2.2053
> progress 1.116%, elapsed 126 s, estimated 11372 s (3 hours), iters = {MDP: 6319}, opt = -2.2053
> progress 1.116%, elapsed 129 s, estimated 11641 s (3 hours), iters = {MDP: 6481}, opt = -2.2053
> progress 1.116%, elapsed 132 s, estimated 11913 s (3 hours), iters = {MDP: 6635}, opt = -2.2053
> progress 1.116%, elapsed 136 s, estimated 12183 s (3 hours), iters = {MDP: 6797}, opt = -2.2053
> progress 1.116%, elapsed 139 s, estimated 12452 s (3 hours), iters = {MDP: 6958}, opt = -2.2053
> progress 1.116%, elapsed 142 s, estimated 12722 s (3 hours), iters = {MDP: 7120}, opt = -2.2053
> progress 1.116%, elapsed 145 s, estimated 12993 s (3 hours), iters = {MDP: 7287}, opt = -2.2053
> progress 1.116%, elapsed 148 s, estimated 13262 s (3 hours), iters = {MDP: 7452}, opt = -2.2053
2025-01-21 17:48:24,348 - synthesizer.py - time limit reached, aborting...
--------------------
Synthesis summary:
optimality objective: R{"reward"}max=? [Cdiscount=99/100] 

method: AR (decision tree), synthesis time: 150.03 s
number of holes: 36, family size: 1e25, quotient: 533 states / 1599 actions
explored: 1 %
MDP stats: avg MDP size: 520, iterations: 7549

optimum: -2.205256
--------------------
2025-01-21 17:48:24,348 - decision_tree.py - families considered: 7549
2025-01-21 17:48:24,348 - decision_tree.py - families skipped by construction: 0
2025-01-21 17:48:24,348 - decision_tree.py - families with schedulers preserved: 2685
2025-01-21 17:48:24,348 - decision_tree.py - families model checked: 4864
2025-01-21 17:48:24,348 - decision_tree.py - harmonizations attempted: 441
2025-01-21 17:48:24,348 - decision_tree.py - harmonizations succeeded: 0


2025-01-21 17:48:24,349 - mdp.py - building tree of depth 4
2025-01-21 17:48:24,521 - statistic.py - synthesis initiated, design space: 1e53
2025-01-21 17:48:25,529 - synthesizer_ar.py - value -2.1859 achieved after 189.69 seconds
> progress 0.0%, elapsed 3 s, estimated 8779188 s (101 days), iters = {MDP: 69}, opt = -2.1859
2025-01-21 17:48:30,535 - synthesizer_ar.py - value -2.1834 achieved after 194.69 seconds
> progress 0.0%, elapsed 6 s, estimated 12920098 s (149 days), iters = {MDP: 137}, opt = -2.1834
2025-01-21 17:48:31,900 - synthesizer_ar.py - value -2.1732 achieved after 196.06 seconds
> progress 0.0%, elapsed 9 s, estimated 17079287 s (197 days), iters = {MDP: 192}, opt = -2.1732
> progress 0.0%, elapsed 12 s, estimated 13372740 s (154 days), iters = {MDP: 261}, opt = -2.1732
> progress 0.0%, elapsed 15 s, estimated 14136463 s (163 days), iters = {MDP: 317}, opt = -2.1732
> progress 0.0%, elapsed 18 s, estimated 12261553 s (141 days), iters = {MDP: 394}, opt = -2.1732
> progress 0.0%, elapsed 21 s, estimated 8523255 s (98 days), iters = {MDP: 475}, opt = -2.1732
> progress 0.0%, elapsed 24 s, estimated 9276438 s (107 days), iters = {MDP: 558}, opt = -2.1732
> progress 0.0%, elapsed 27 s, estimated 10122208 s (117 days), iters = {MDP: 638}, opt = -2.1732
> progress 0.0%, elapsed 30 s, estimated 10805886 s (125 days), iters = {MDP: 710}, opt = -2.1732
> progress 0.0%, elapsed 33 s, estimated 11781336 s (136 days), iters = {MDP: 760}, opt = -2.1732
> progress 0.0%, elapsed 36 s, estimated 12395185 s (143 days), iters = {MDP: 817}, opt = -2.1732
> progress 0.0%, elapsed 39 s, estimated 13225428 s (153 days), iters = {MDP: 896}, opt = -2.1732
> progress 0.0%, elapsed 42 s, estimated 13920111 s (161 days), iters = {MDP: 978}, opt = -2.1732
> progress 0.0%, elapsed 45 s, estimated 11095319 s (128 days), iters = {MDP: 1058}, opt = -2.1732
> progress 0.0%, elapsed 48 s, estimated 10218791 s (118 days), iters = {MDP: 1149}, opt = -2.1732
--------------------
Synthesis summary:
optimality objective: R{"reward"}max=? [Cdiscount=99/100] 

method: AR (decision tree), synthesis time: 49.12 s
number of holes: 76, family size: 1e53, quotient: 533 states / 1599 actions
explored: 100 %
MDP stats: avg MDP size: 533, iterations: 1162

optimum: -2.173194
--------------------
2025-01-21 17:49:13,645 - decision_tree.py - families considered: 1162
2025-01-21 17:49:13,645 - decision_tree.py - families skipped by construction: 0
2025-01-21 17:49:13,645 - decision_tree.py - families with schedulers preserved: 450
2025-01-21 17:49:13,645 - decision_tree.py - families model checked: 712
2025-01-21 17:49:13,645 - decision_tree.py - harmonizations attempted: 12
2025-01-21 17:49:13,645 - decision_tree.py - harmonizations succeeded: 0

2025-01-21 17:49:13,645 - decision_tree.py - printing synthesized assignment below:
2025-01-21 17:49:13,645 - decision_tree.py - V_0=dealer_total, dealer_total_0=6, player_total_0=0, skipped_0=0, V_1=player_total, dealer_total_1=0, player_total_1=11, skipped_1=0, V_2=player_total, dealer_total_2=0, player_total_2=11, skipped_2=0, V_3=dealer_total, dealer_total_3=0, player_total_3=0, skipped_3=0, A_4=(Skip), A_5=(Draw), V_6=dealer_total, dealer_total_6=3, player_total_6=0, skipped_6=0, A_7=(Skip), A_8=(Draw), V_9=skipped, dealer_total_9=0, player_total_9=0, skipped_9=0, V_10=skipped, dealer_total_10=0, player_total_10=0, skipped_10=0, A_11=(Skip), A_12=(Draw), V_13=skipped, dealer_total_13=0, player_total_13=0, skipped_13=0, A_14=(Draw), A_15=(Skip), V_16=player_total, dealer_total_16=0, player_total_16=13, skipped_16=0, V_17=dealer_total, dealer_total_17=10, player_total_17=0, skipped_17=0, V_18=skipped, dealer_total_18=6, player_total_18=12, skipped_18=0, A_19=(Draw), A_20=(Skip), V_21=player_total, dealer_total_21=11, player_total_21=11, skipped_21=0, A_22=(Draw), A_23=(Skip), V_24=dealer_total, dealer_total_24=9, player_total_24=0, skipped_24=0, V_25=player_total, dealer_total_25=0, player_total_25=14, skipped_25=0, A_26=(Draw), A_27=(Skip), V_28=player_total, dealer_total_28=0, player_total_28=0, skipped_28=0, A_29=(Skip), A_30=(Skip)
2025-01-21 17:49:13,647 - decision_tree.py - double-checking specification satisfiability:  : -2.1731936652535744
2025-01-21 17:49:13,647 - decision_tree.py - the optimal scheduler has value: -2.1731936652535744
2025-01-21 17:49:13,647 - decision_tree.py - the random scheduler has value: -11.731985750064817
2025-01-21 17:49:13,648 - decision_tree.py - synthesized tree of depth 4 with 9 decision nodes
2025-01-21 17:49:13,648 - decision_tree.py - the synthesized tree has value -2.1731936652535744
2025-01-21 17:49:13,648 - decision_tree.py - the synthesized tree has relative value: 1.0
2025-01-21 17:49:13,648 - decision_tree.py - printing the synthesized tree below:
if dealer_total<=6:
  if player_total<=11:
    if dealer_total<=0:
      (Skip)
    else:
      (Draw)
  else:
    (Skip)
else:
  if player_total<=13:
    if dealer_total<=10:
      if skipped<=0:
        (Draw)
      else:
        (Skip)
    else:
      if player_total<=11:
        (Draw)
      else:
        (Skip)
  else:
    if dealer_total<=9:
      if player_total<=14:
        (Draw)
      else:
        (Skip)
    else:
      (Skip)

2025-01-21 17:49:13,648 - decision_tree.py - exported decision tree to logs/01-21-all/4/omdt-blackjack/tree.dot
2025-01-21 17:49:13,708 - decision_tree.py - exported decision tree visualization to logs/01-21-all/4/omdt-blackjack/tree.png
2025-01-21 17:49:13,708 - decision_tree.py - synthesis finished after 237.87 seconds

ColoringSmt = 0.156 s (0.1 %)
ColoringSmt:: create choice colors = 0.037 s (0.0 %)
ColoringSmt:: create harmonizing variants = 0.118 s (0.0 %)
areChoicesConsistent = 44.132 s (18.6 %)
areChoicesConsistent::1 is scheduler consistent? = 10.809 s (4.5 %)
areChoicesConsistent::2 better unsat core = 31.825 s (13.4 %)
areChoicesConsistent::3 unsat core analysis = 0.95 s (0.4 %)
check = 35.957 s (15.1 %)
loadUnsatCore = 0.005 s (0.0 %)
selectCompatibleChoices = 2.626 s (1.1 %)
selectCompatibleChoices::1 is family sat = 2.149 s (0.9 %)
selectCompatibleChoices::2 state exploration = 0.476 s (0.2 %)
